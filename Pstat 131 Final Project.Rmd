---
title: "Pstat 131 Final Project"
author: "Daniel Pry, Nick Van Daelen, Rick Zheng"
date: "2/27/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ISLR)
library(glmnet)
library(tree)
library(maptree)
library(randomForest)
library(gbm)
library(ROCR)
library(ggplot2)
library(ROSE)
library(class)
library(reshape2)
library(psych)
library(caret)
library(e1071)
library(MASS)
```

### Pstat 131 Winter 2022 Final Project:

# Predicting if NCAA Basketball players will be drafted to the NBA based on their college stats.

# Introduction



# Loading Data

Loading Data Set:
source: https://www.kaggle.com/adityak2003/college-basketball-players-20092021
```{r}
college_stats <- read.csv('CollegeBasketballPlayers2009-2021.csv',
                          na.strings = c("", "N/A", "None", "-"))
```


# Data Cleaning

```{r}
stats <- college_stats %>% mutate(drafted = ifelse(is.na(pick), 0, 1))
```

```{r}
s <- stats %>% group_by(pid) %>% summarise(player_name = player_name, team = team, conf = conf, seasons = length(pid), Min_per  = Min_per, Ortg  = Ortg, usg  = usg, eFG  = eFG, TS_per  = TS_per, ORB_per  = ORB_per, DRB_per  = DRB_per, TO_per  = TO_per, AST_per  = AST_per, FTM  = FTM, FTA  = FTA, FT_per = FT_per, twoPM = twoPM, twoPA = twoPA, twoP_per = twoP_per, TPM = TPM, TPA = TPA, TP_per = TP_per, blk_per = blk_per, stl_per = stl_per, college_year = yr, ht = ht, adjoe  = adjoe, year = year, pid = pid, Rec.Rank = Rec.Rank, ast.tov = ast.tov, rimmade = rimmade, rim_att = rimmade.rimmiss, rim_per = rimmade..rimmade.rimmiss., midmade = midmade, mid_att = midmade.midmiss, mid_per = midmade..midmade.midmiss., dunksmade = dunksmade, dunks_att = dunksmiss.dunksmade, dunk_per = dunksmade..dunksmade.dunksmiss., drtg = drtg, adrtg = adrtg, stops = stops, bpm = bpm, min_played = mp, off_reb = oreb, def_reb = dreb, total_reb = treb, ast = ast, stl = stl, blk = blk, pts = pts, Position = X, drafted = drafted)

s_cleaned <- s %>% slice_max(year)
head(s_cleaned)
```

Cleaning abnormal values in data set
```{r}
s_cleaned[, 27][s_cleaned[, 27] == 0] <- NA
s_cleaned[, 27][s_cleaned[, 27] == "So"] <- NA
s_cleaned[, 27][s_cleaned[, 27] == "Jr"] <- NA
s_cleaned[, 26][s_cleaned[, 26] == 0] <- NA
```


```{r}
#changing ht
s_cleaned %>% dplyr::select(ht)
begin_height <- data.frame(do.call("rbind", strsplit(as.character(s_cleaned$ht), "-", fixed = TRUE)))

```

```{r, fig.width=10}
begin_height["X1"][begin_height["X1"] == "Jun"] <- 72
begin_height["X1"][begin_height["X1"] == "Jul"] <- 84
begin_height["X1"][begin_height["X1"] == "May"] <- 60
begin_height["X2"][begin_height["X2"] == "Jun"] <- 72
begin_height["X2"][begin_height["X2"] == "Jul"] <- 84
begin_height["X2"][begin_height["X2"] == "May"] <- 60
begin_height$X1 <- as.numeric(as.character(begin_height$X1))
begin_height$X2 <- as.numeric(as.character(begin_height$X2))
begin_height$height <- begin_height$X1 + begin_height$X2

df_new <- cbind(s_cleaned, begin_height)

df_new %>% dplyr::select(-c('X1', 'X2', 'ht'))
df_new$height <- df_new$height %>% as.numeric() %>% replace_na(76.27)
df_new %>% dplyr::select(height)
s_cleaned <- df_new
```
Lastly, to finish cleaning our data we must get rid of players in the data who have not yet had the chance to either be drafted or to not be drafted. These players who are still playing in college will not help our model. A very good active college player would, as of now, have a zero in the "drafted" column. This could potentially weaken our model as they may go on to be drafted in the future. THerefore, we remove players whose "year" is 2021.

```{r}
s_cleaned <- subset(s_cleaned, year != 2021)

```

## Exploratory Data Analysis

This histogram shows the average points scored per game on the x-axis and the frequency number of players in the data set that score average that number of points. 
```{r}
ggplot(s_cleaned, aes(x = pts)) + 
  geom_histogram()
```


In the following plot we compare the averages of certain predictors for drafted vs non-drafted players. From these it is possible to make potential assumptions on what predictors might be significant for predicting if a player is drafted. For example the difference in average height of drafted and non-drafted is not very different so this might imply that height will not be a very significant predictor in many of the models. Many of the other predictors do have very different averages for drafted and non-drafted players so it is very likely some of these other predictors will be signifcant in predicting if a player is drafted. 
```{r}
# group by drafted or not
drafted_means <- s_cleaned %>% subset(drafted == 1) %>% sapply(mean, na.rm=TRUE)
drafted_means <- as.data.frame(drafted_means)

undrafted_means <- s_cleaned %>% subset(drafted == 0) %>% sapply(mean, na.rm=TRUE)
undrafted_means <- as.data.frame(undrafted_means)

# taking averages of drafted and undrafted players
drafted_vs_undrafted <- undrafted_means %>% mutate(drafted_means)

# transforming data frame for plotting
drafted_vs_undrafted$names <- rownames(drafted_vs_undrafted)
rownames(drafted_vs_undrafted) <- NULL
drafted_vs_undrafted <- gather(drafted_vs_undrafted, event, total, drafted_means:undrafted_means)

d_vs_u <- drafted_vs_undrafted %>% filter(names == "pts" | names == "dunksmade" |
                                            names == "min_played" | names == "ast" | 
                                            names == "TPM" | names == "midmade"| 
                                            names == "blk" | names == "total_reb" | 
                                            names == "twoPM" | names == "rimmade" |
                                            names == "stl" | names == "height")

# plotting of means
avg_bars <- d_vs_u %>% ggplot(aes(names, total, fill=event)) + 
  geom_bar(stat = "identity", position = 'dodge') + facet_wrap(~ names, scales = "free")
avg_bars
```


Making a barplot of the percentage of players from each conference that are drafted.

We notice that there are a handful of conferences that have a drastically higher percentage of players drafted. Additionally, there are a few conferences where no players have been drafted.
```{r, fig.width=10}
draft_by_conf <-  s_cleaned %>% group_by(conf) %>% summarise(prop_drafted = mean(drafted))

ggplot(draft_by_conf) + geom_bar(aes(x = conf, y = prop_drafted), stat = "identity") +
  theme(axis.text.x = element_text(angle = 90))
```

This plot exposes a few problems with the naming of the conferences in the Data. The independent conference is listed as 2 different conferences (ind and Ind) as a result of a discrepency in the capitalization and the PAC-12 was formerly known as the PAC-10 which is why there is a P10 as well as a P12. We fix these issues below and reprint the bar graph. 
```{r}
s_cleaned["conf"][s_cleaned["conf"] == "ind"] <- "Ind"
s_cleaned["conf"][s_cleaned["conf"] == "P10"] <- "P12"
```

```{r}
draft_by_conf <-  s_cleaned %>% group_by(conf) %>% summarise(prop_drafted = mean(drafted))

ggplot(draft_by_conf) + geom_bar(aes(x = conf, y = prop_drafted), stat = "identity") +
  theme(axis.text.x = element_text(angle = 90))
```

To better understand the relationship between our predictors, we make scatteplots with 2 predictors on the axis and color indicating wheter that player was drafted.

This first scatterplot has minutes played on the x axis and Points on the y axis. As expected these two variables are highly positively correlated with players who would go on to be drafted having higer values of both ending up in the top right of this plot.
```{r}
s_cleaned %>% ggplot(aes(x=min_played, y = pts, col = drafted)) + geom_point()

```

We made another scatterplot to examine the relationship between Dunks made and three pointers made. 

```{r}
s_cleaned %>% ggplot(aes(x=dunksmade, y = TPM, col = drafted)) + geom_point()


```

The metrics of assists (ast) and assist percentage are base off of similar stats and therefore we expect them to be highly correlated. We wonder if they should both be included in the model as predictors. We plot a scatterplot of the data on these two variables below.

```{r}

s_cleaned %>% ggplot(aes(x=AST_per, y = ast, col = drafted)) + geom_point()

```
From the scatterplot we can tell that the y axis (ast) is more informative to a players draft status than the x axis (ast_per) is. There are so many players that dont play all that often and could, as a result of sampling variation have a high assist percentage. None of these players wind up getting drafted. Therefore, we believe it is best to use assists as a predictor rather than assist percentage because the two metrics are highly correlated for players with a significant amount of data, but just looking at assist percentage could be misleading because of the players that aren't particularly good but still have a high assist percentage due to their small sample size.


```{r}
s_cleaned %>% ggplot(aes(x=rimmade, y = rim_per, col = drafted)) + geom_point()


```


# Test/training split

In order to simplify our code when we fit our models later, we start by simplifying our dataset to only include certain columns that will be used as predictors. Additionaly, categorical predictors such as conference, seasons, and drafted must be converted to factors.

```{r}
sc <- s_cleaned
sc$conference <- as.factor(s_cleaned$conf)
sc$num_seasons <- as.factor(s_cleaned$seasons)
sc$drafted <- factor(s_cleaned$drafted, levels = c(0,1))


stats_df <- sc %>% ungroup() %>% dplyr::select(c(pid, year, conference, num_seasons, Min_per, usg, FTM, FT_per, twoPM, twoP_per, TPM, TP_per, adjoe, rimmade, rim_per, midmade, mid_per, dunksmade, dunk_per, stops, min_played, off_reb, def_reb, total_reb, ast, stl, blk, pts, height, drafted))

```



Random forests cannot contain missing values in predictor columns, therefore the columns with many missing values (rimmade, midmade, dunksmade, rim_per, mid_per, and dunk_per) will have their missing values replaced with 0.

```{r}

sapply(stats_df, function(x) sum(is.na(x)))
```

```{r}

stats_df[is.na(stats_df)] <- 0

sapply(stats_df, function(x) sum(is.na(x)))

```


We will split the data into a test set and a training set. Since we have a relatively large number of observations, we should have a large enough test set if we used only 15 percent of the data for the test set, leaving many observations in the training set to fit our models.

We sample for our test set using a stratified random sample. This is a good idea in this case because ...

```{r}
set.seed(123)

test_set <- stats_df %>% group_by(year) %>% sample_frac(size = 0.15)

test_set <- test_set[order(test_set$pid),]


training_set <- stats_df[!stats_df$pid %in% test_set$pid,]


table(test_set$drafted)
table(training_set$drafted)
```
Now that we've split the data into test and training sets, we can remove the year column as it will not be used as a predictor in the model. Player id is also no longer necessary. Both of these columns will be removed from both the training set as well as the test set

```{r}
training_set <- training_set %>% ungroup()

test_set <- test_set %>% ungroup()

training_set <- dplyr::select(training_set, -c(pid,year))

test_set <- dplyr::select(test_set, -c(pid,year))

```

The two classes we have (Drafted vs not drafted) are extremely imbalanced. Less than 3 percent of the players in out data get drafted. This is a problem because if left this way our models will incorrectly classify many of the drafted players as not drafted. It could even have a relatively low error rate by simply classifying every observation as not drafted. Something needs to be done about this.

In order to mitigate this problem we will employ sampling techniques using the ovun_sample() function. In this case we will both oversample and undersample. The minority class(drafted players) will be oversampled with replacement while the majority class (undrafted players) will be undersampled with replacement. We will keep the same number of datapoints as there were in the origional training set.

```{r}
set.seed(112)
balanced_train_set <- ovun.sample(drafted ~ ., data = training_set, method = "both", p=0.3,                             N=17658, seed = 1)$data

table(balanced_train_set$drafted)
```

# Model Fitting

Our process for fitting the models will follow the following steps:

1. First we'll fit an initial models using parameters that seem intuitive.

2. Next, we'll use cross validation to tune those parameters to find the best fit

3. Then we will fit a model using the optimal parameters we found and analyze each of the models using some sort of visualization.

*Random Forest:*

Fit an initial random forrest model. Here I use mtry = 5 as it is close to the square root of the number of predictors we have. We'll use cross validation later to determine the optimal number for this.

```{r}
rf.bball = randomForest(drafted ~ conference + num_seasons + Min_per + usg + FTM + FT_per + twoPM + twoP_per + TPM + TP_per + adjoe + rimmade + rim_per + midmade + mid_per + dunksmade + dunk_per + stops + min_played + off_reb + def_reb + total_reb + ast + stl + blk + pts + height, data=balanced_train_set, mtry=5, ntree=300, importance=TRUE)

rf.bball

plot(rf.bball)


```

To cross validate we will loop through all the reasonable values for m. We fit a model with each of the m's and record the out-of-bag error rate. We also make note of the classification error for the true values because if we are trying to predict which players get drafted, it is important to correctly classify as many of the drafted players as possible. Since the proportion of players that get drafted is small, minimizing the overall error might not do the best job at correctly classifying as many of the drafted players as possible. 

```{r}
m = c()
tce = c()
fce = c()
oob = c()

ntree = 300
for (i in 2:15){
  rf = randomForest(drafted ~ conference + num_seasons + Min_per + usg + FTM + FT_per + twoPM + twoP_per + TPM + TP_per + adjoe + rimmade + rim_per + midmade + mid_per + dunksmade + dunk_per + stops + min_played + off_reb + def_reb + total_reb + ast + stl + blk + pts + height, data=balanced_train_set, mtry=i, ntree=ntree, importance=TRUE)
  
  m[i-1] <- i
  tce[i-1] <- rf$confusion[2,3]
  oob[i-1] <- rf$err.rate[ntree, 1]
  fce[i-1] <- rf$confusion[1,3]
}
err = data.frame(cbind(m=m, class_error_true = tce, class_error_false = fce, out_of_bag = oob))

small_err <- err %>% filter(out_of_bag==min(out_of_bag))

m_optimal = max(small_err$m)
```

It looks like 
We fit a model with this parameter and plot the results below.
```{r}
rf_opt = randomForest(drafted ~ conference + num_seasons + Min_per + usg + FTM + FT_per + twoPM + twoP_per + TPM + TP_per + adjoe + rimmade + rim_per + midmade + mid_per + dunksmade + dunk_per + stops + min_played + off_reb + def_reb + total_reb + ast + stl + blk + pts + height, data=balanced_train_set, mtry=m_optimal, ntree=ntree, importance=TRUE)

rf_opt

plot(rf_opt)
```

```{r, fig.height=12, fig.width=12}
varImpPlot(rf_opt)

```


*Logistic Regression:*

We'll Start by fitting a logistic regression model with all of the predictors.

```{r}
full_model <- glm(drafted ~ ., data = balanced_train_set, family = 'binomial')
summary(full_model)


```

We notice that there are definitely some predictors that are more useful than others. We will use backward stepwise selection to determine the best subset of predictors for our model.

```{r}
step_model <- full_model %>% stepAIC(trace = FALSE)
summary(step_model)

```

It looks like our code dropped a few of the predictors to make a better model. We notice that the AIC for the step_model is lower than it was in the full_model which is a good thing.

Next, we must determine which threshhold is best to use to make predictions. We'll loop through all of the reasonable threshholds and then record the true positive rate, false positive rate, and total error for each. We'll use these metrics to decide on which threshold is best.

```{r}
prob.training = predict(step_model, type="response")



t <- c()
tpr <- c()
fpr <- c()
error <- c()
  
for (i in 1:9){
  
  bal_trn_st_w_pred = balanced_train_set %>% mutate(pred_drafted=as.factor(ifelse(prob.training<=(i/10), "No", "Yes")))

  con_mat <- table(pred=bal_trn_st_w_pred$pred_drafted, true=balanced_train_set$drafted)
  
  t[i] <- i/10
  tpr[i] <- con_mat[2,2]/(con_mat[2,2] + con_mat[1,2])
  fpr[i] <- con_mat[2,1]/(con_mat[2,1] + con_mat[1,1])
  error[i] <- (con_mat[1, 2] + con_mat[2,1])/(con_mat[1, 2] + con_mat[2,1] + con_mat[2, 2] + con_mat[1,1])
}
err_logit = data.frame(cbind(t=t, true_pos_rate = tpr, false_pos_rate = fpr, total_error = error))

err_logit

small_err_logit <- err_logit %>% filter(total_error==min(total_error))

optimal_threshold = max(small_err_logit$t)
```

Constructing and ROC Curve

```{r}
pred = prediction(prob.training, balanced_train_set$drafted)

perf = performance(pred, measure="tpr", x.measure="fpr")

plot(perf, col=2, lwd=3, main="ROC curve")
abline(0,1)

```

*Boosted Trees:*

```{r}
install.packages("rpart.plot")
library(rpart)
library(rpart.plot)

#rpart(formula, data = , method = "")

boosted_fit <- rpart(drafted ~., data = balanced_train_set, method = "class")

rpart.plot(boosted_fit, extra = 106)

```

```{r}
boosted_predict <- predict(boosted_fit, test_set, type = "class")

table_boosted <- table(test_set$drafted, boosted_predict)
table_boosted

accuracy_boosted <- sum(diag(table_boosted)) / sum(table_boosted)
print(paste('Boosted Trees: Accuracy', accuracy_boosted))

```

```{r}
#hypertuning


hypertune_boosted <- function(boosted_fit) {
    new_predict <- predict(boosted_fit, test_set, type = 'class')
    new_table <- table(test_set$drafted, new_predict)
    new_accuracy <- sum(diag(new_table)) / sum(new_table)
    new_accuracy
}

tuning <- rpart.control(minsplit = 6, minbucket = round(2/3), maxdepth = 10, cp = 0)
#minsplit is min number of obs in node before next split
#minbucket is min number of obs in leaf node
#maxdepth is max depth of tree

tune_fit <- rpart(drafted~., data = balanced_train_set, method = 'class', control = tuning)

accuracy_tune(tune_fit)

```


*K-Nearest-Neighbors*

```{r}
str(training_set)

table(training_set$num_seasons)

table(training_set$conference)
```
Before we begin, we have to do something about the categorical variables we are using as predictors. A k-nearest-neighbors model would not work with variables like conference and num_seasons as we currently have them. We will dummycode these variables in both the training set as well as the test set.

```{r}
conference <- as.data.frame(dummy.code(balanced_train_set$conference))
num_seasons <- as.data.frame(dummy.code(balanced_train_set$num_seasons))

conference_t <- as.data.frame(dummy.code(test_set$conference))
num_seasons_t <- as.data.frame(dummy.code(test_set$num_seasons))


trn_st <- cbind(balanced_train_set, conference, num_seasons)

trn_st <- trn_st %>% dplyr::select(-one_of(c("conference", "num_seasons")))



tst_st <- cbind(test_set, conference_t, num_seasons_t)

tst_st <- tst_st %>% dplyr::select(-one_of(c("conference", "num_seasons")))
```


```{r}

# YTrain is the true labels for drafted on the training set, XTrain is the design matrix
YTrain = trn_st$drafted
XTrain = trn_st %>% dplyr::select(-drafted) %>% scale(center = TRUE, scale = TRUE)
# YTest is the true labels for drafted on the test set, Xtest is the design matrix
YTest = tst_st$drafted
XTest = tst_st %>% dplyr::select(-drafted) %>% scale(center = TRUE, scale = TRUE)


```




```{r}
do.chunk <- function(chunkid, folddef, Xdat, Ydat, ...){
  # Get training index
  train = (folddef!=chunkid)
  # Get training set by the above index
  Xtr = Xdat[train,]
  # Get responses in training set
  Ytr = Ydat[train]
  # Get validation set
  Xvl = Xdat[!train,]
  # Get responses in validation set
  Yvl = Ydat[!train]
  # Predict training labels
  predYtr = knn(train=Xtr, test=Xtr, cl=Ytr, ...)
  # Predict validation labels
  predYvl = knn(train=Xtr, test=Xvl, cl=Ytr, ...)
  data.frame(fold = chunkid,
  train.error = mean(predYtr != Ytr), # Training error for each fold
  val.error = mean(predYvl != Yvl)) # Validation error for each fold
}

```



Here we execute the cross validation using the do.chunk function we created above. We will take the averages of the validation error for each value for k and store the lowest on as a variable called k_optimal
```{r}
nfold = 3
folds = cut(1:nrow(trn_st), breaks=nfold, labels=FALSE) %>% sample()



error.folds = NULL
# Give possible number of nearest neighbours to be considered
allK = 3:5
# Loop through different number of neighbors
for (k in allK){
# Loop through different chunk id
  for (j in seq(nfold)){
    tmp = do.chunk(chunkid=j, folddef=folds, Xdat=XTrain, Ydat=YTrain, k=k)
    tmp$neighbors = k # Record the last number of neighbor
    error.folds = rbind(error.folds, tmp) # combine results
  }
}
head(error.folds, 10)


smallest_err <- error.folds %>% group_by(neighbors) %>% summarise(neighbors = neighbors, avg_val.err = mean(val.error)) %>% ungroup() %>% filter(avg_val.err==min(avg_val.err))

k_optimal = max(smallest_err$neighbors)

```

This values for k_optimal is what we will use to evaluate the performance of this model and compare this k-nearest-neighbor model to the other models that we fit.




# Model perforemance and Selection





# Conclusion