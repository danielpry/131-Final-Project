---
title: "Pstat 131 Final Project"
author: "Daniel Pry, Nick Van Daelen, Rick Zheng"
date: "2/27/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ISLR)
library(glmnet)
library(tree)
library(maptree)
library(randomForest)
library(gbm)
library(ROCR)
library(ggplot2)
library(ROSE)
library(class)
library(reshape2)
library(psych)
library(caret)
library(e1071)
library(MASS)
library(rpart)
library(rpart.plot)
library(skimr)
```

### Pstat 131 Winter 2022 Final Project:

# Predicting if NCAA Basketball players will be drafted to the NBA based on their college stats.

# Introduction

Every year the NBA holds their draft, where teams will pick to sign new players on to their team. Per the NBA rules for the draft there are not many rules against who can’t be drafted other than the player must be at least one year removed from high school. Naturally this causes NCAA basketball teams to be the biggest pool of talent for the NBA to draft athletes from. Historically about ninety percent of players drafted come from Division one programs in the NCAA. In this project we will be attempting to use players’ statistics from their last season in the NCAA from the years 2009 to 2020 to predict if a player will be drafted. The data set includes about 20000 players where only about 500 end up getting drafted. 


# Loading Data

Loading Data Set:
source: https://www.kaggle.com/adityak2003/college-basketball-players-20092021
```{r}
college_stats <- read.csv('CollegeBasketballPlayers2009-2021.csv',
                          na.strings = c("", "N/A", "None", "-"))
```

\newpage

# Data Cleaning

```{r}
stats <- college_stats %>% mutate(drafted = ifelse(is.na(pick), 0, 1))
```

```{r}
s <- stats %>% group_by(pid) %>% summarise(player_name = player_name, team = team, conf = conf, seasons = length(pid), Min_per  = Min_per, Ortg  = Ortg, usg  = usg, eFG  = eFG, TS_per  = TS_per, ORB_per  = ORB_per, DRB_per  = DRB_per, TO_per  = TO_per, AST_per  = AST_per, FTM  = FTM, FTA  = FTA, FT_per = FT_per, twoPM = twoPM, twoPA = twoPA, twoP_per = twoP_per, TPM = TPM, TPA = TPA, TP_per = TP_per, blk_per = blk_per, stl_per = stl_per, college_year = yr, ht = ht, adjoe  = adjoe, year = year, pid = pid, Rec.Rank = Rec.Rank, ast.tov = ast.tov, rimmade = rimmade, rim_att = rimmade.rimmiss, rim_per = rimmade..rimmade.rimmiss., midmade = midmade, mid_att = midmade.midmiss, mid_per = midmade..midmade.midmiss., dunksmade = dunksmade, dunks_att = dunksmiss.dunksmade, dunk_per = dunksmade..dunksmade.dunksmiss., drtg = drtg, adrtg = adrtg, stops = stops, bpm = bpm, min_played = mp, off_reb = oreb, def_reb = dreb, total_reb = treb, ast = ast, stl = stl, blk = blk, pts = pts, Position = X, drafted = drafted)

s_cleaned <- s %>% slice_max(year)
head(s_cleaned)
```

Cleaning abnormal values in data set
```{r}
s_cleaned[, 27][s_cleaned[, 27] == 0] <- NA
s_cleaned[, 27][s_cleaned[, 27] == "So"] <- NA
s_cleaned[, 27][s_cleaned[, 27] == "Jr"] <- NA
s_cleaned[, 26][s_cleaned[, 26] == 0] <- NA
```


```{r}
#changing ht
s_cleaned %>% dplyr::select(ht)
begin_height <- data.frame(do.call("rbind", strsplit(as.character(s_cleaned$ht), "-", fixed = TRUE)))

```

```{r, fig.width=10}
begin_height["X1"][begin_height["X1"] == "Jun"] <- 72
begin_height["X1"][begin_height["X1"] == "Jul"] <- 84
begin_height["X1"][begin_height["X1"] == "May"] <- 60
begin_height["X2"][begin_height["X2"] == "Jun"] <- 72
begin_height["X2"][begin_height["X2"] == "Jul"] <- 84
begin_height["X2"][begin_height["X2"] == "May"] <- 60
begin_height$X1 <- as.numeric(as.character(begin_height$X1))
begin_height$X2 <- as.numeric(as.character(begin_height$X2))
begin_height$height <- begin_height$X1 + begin_height$X2

df_new <- cbind(s_cleaned, begin_height)

df_new %>% dplyr::select(-c('X1', 'X2', 'ht'))
df_new$height <- df_new$height %>% as.numeric() %>% replace_na(76.27)
df_new %>% dplyr::select(height)
s_cleaned <- df_new
```
Lastly, to finish cleaning our data we must get rid of players in the data who have not yet had the chance to either be drafted or to not be drafted. These players who are still playing in college will not help our model. A very good active college player would, as of now, have a zero in the "drafted" column. This could potentially weaken our model as they may go on to be drafted in the future. THerefore, we remove players whose "year" is 2021.

```{r}
s_cleaned <- subset(s_cleaned, year != 2021)

```

\newpage

## Exploratory Data Analysis

This histogram shows the average points scored per game on the x-axis and the frequency number of players in the data set that score average that number of points. 
```{r}
ggplot(s_cleaned, aes(x = pts)) + 
  geom_histogram()
```


In the following plot we compare the averages of certain predictors for drafted vs non-drafted players. From these it is possible to make potential assumptions on what predictors might be significant for predicting if a player is drafted. For example the difference in average height of drafted and non-drafted is not very different so this might imply that height will not be a very significant predictor in many of the models. Many of the other predictors do have very different averages for drafted and non-drafted players so it is very likely some of these other predictors will be signifcant in predicting if a player is drafted. 
```{r}
# group by drafted or not
drafted_means <- s_cleaned %>% subset(drafted == 1) %>% sapply(mean, na.rm=TRUE)
drafted_means <- as.data.frame(drafted_means)

undrafted_means <- s_cleaned %>% subset(drafted == 0) %>% sapply(mean, na.rm=TRUE)
undrafted_means <- as.data.frame(undrafted_means)

# taking averages of drafted and undrafted players
drafted_vs_undrafted <- undrafted_means %>% mutate(drafted_means)

# transforming data frame for plotting
drafted_vs_undrafted$names <- rownames(drafted_vs_undrafted)
rownames(drafted_vs_undrafted) <- NULL
drafted_vs_undrafted <- gather(drafted_vs_undrafted, event, total, drafted_means:undrafted_means)

d_vs_u <- drafted_vs_undrafted %>% filter(names == "pts" | names == "dunksmade" |
                                            names == "min_played" | names == "ast" | 
                                            names == "TPM" | names == "midmade"| 
                                            names == "blk" | names == "total_reb" | 
                                            names == "twoPM" | names == "rimmade" |
                                            names == "stl" | names == "height")

# plotting of means
avg_bars <- d_vs_u %>% ggplot(aes(names, total, fill=event)) + 
  geom_bar(stat = "identity", position = 'dodge') + facet_wrap(~ names, scales = "free")
avg_bars
```


Making a barplot of the percentage of players from each conference that are drafted.

We notice that there are a handful of conferences that have a drastically higher percentage of players drafted. Additionally, there are a few conferences where no players have been drafted.
```{r, fig.width=10}
draft_by_conf <-  s_cleaned %>% group_by(conf) %>% summarise(prop_drafted = mean(drafted))

ggplot(draft_by_conf) + geom_bar(aes(x = conf, y = prop_drafted), stat = "identity") +
  theme(axis.text.x = element_text(angle = 90))
```

This plot exposes a few problems with the naming of the conferences in the Data. The independent conference is listed as 2 different conferences (ind and Ind) as a result of a discrepency in the capitalization and the PAC-12 was formerly known as the PAC-10 which is why there is a P10 as well as a P12. We fix these issues below and reprint the bar graph. 
```{r}
s_cleaned["conf"][s_cleaned["conf"] == "ind"] <- "Ind"
s_cleaned["conf"][s_cleaned["conf"] == "P10"] <- "P12"
```

```{r}
draft_by_conf <-  s_cleaned %>% group_by(conf) %>% summarise(prop_drafted = mean(drafted))

ggplot(draft_by_conf) + geom_bar(aes(x = conf, y = prop_drafted), stat = "identity") +
  theme(axis.text.x = element_text(angle = 90))
```

To better understand the relationship between our predictors, we make scatteplots with 2 predictors on the axis and color indicating wheter that player was drafted.

This first scatterplot has minutes played on the x axis and Points on the y axis. As expected these two variables are highly positively correlated with players who would go on to be drafted having higer values of both ending up in the top right of this plot.
```{r}
s_cleaned %>% ggplot(aes(x=min_played, y = pts, col = drafted)) + geom_point()

```

We made another scatterplot to examine the relationship between Dunks made and three pointers made. 

```{r}
s_cleaned %>% ggplot(aes(x=dunksmade, y = TPM, col = drafted)) + geom_point()

```

The metrics of assists (ast) and assist percentage are base off of similar stats and therefore we expect them to be highly correlated. We wonder if they should both be included in the model as predictors. We plot a scatterplot of the data on these two variables below.

```{r}

s_cleaned %>% ggplot(aes(x=AST_per, y = ast, col = drafted)) + geom_point()

```
From the scatterplot we can tell that the y axis (ast) is more informative to a players draft status than the x axis (ast_per) is. There are so many players that dont play all that often and could, as a result of sampling variation have a high assist percentage. None of these players wind up getting drafted. Therefore, we believe it is best to use assists as a predictor rather than assist percentage because the two metrics are highly correlated for players with a significant amount of data, but just looking at assist percentage could be misleading because of the players that aren't particularly good but still have a high assist percentage due to their small sample size.


```{r}
s_cleaned %>% ggplot(aes(x=rimmade, y = rim_per, col = drafted)) + geom_point()
```

```{r}
s_cleaned2 <- na.omit(s_cleaned)
data <- cor(s_cleaned2[sapply(s_cleaned2,is.numeric)])
 
data1 <- melt(data)
data1 <- data1 %>% filter(value < 1)
data2 <- data1[order(data1$value),]
           
colnames(data2) <- c("Variable1", "Variable2", "Correlation")            
head(data2, 10)
```

\newpage

# Test/training split

In order to simplify our code when we fit our models later, we start by simplifying our dataset to only include certain columns that will be used as predictors. Additionaly, categorical predictors such as conference, seasons, and drafted must be converted to factors.

```{r}
sc <- s_cleaned
sc$conference <- as.factor(s_cleaned$conf)
sc$num_seasons <- as.factor(s_cleaned$seasons)
sc$drafted <- factor(s_cleaned$drafted, levels = c(0,1))


stats_df <- sc %>% ungroup() %>% dplyr::select(c(pid, year, conference, num_seasons, Min_per, usg, FTM, FT_per, twoPM, twoP_per, TPM, TP_per, adjoe, rimmade, rim_per, midmade, mid_per, dunksmade, dunk_per, stops, min_played, off_reb, def_reb, total_reb, ast, stl, blk, pts, height, drafted))

```



Random forests cannot contain missing values in predictor columns, therefore the columns with many missing values (rimmade, midmade, dunksmade, rim_per, mid_per, and dunk_per) will have their missing values replaced with 0.

```{r}

sapply(stats_df, function(x) sum(is.na(x)))
```

```{r}

stats_df[is.na(stats_df)] <- 0

sapply(stats_df, function(x) sum(is.na(x)))

```


We will split the data into a test set and a training set. Since we have a relatively large number of observations, we should have a large enough test set if we used only 15 percent of the data for the test set, leaving many observations in the training set to fit our models.

We sample for our test set using a stratified random sample. This is a good idea in this case because ...

```{r}
set.seed(123)

test_set <- stats_df %>% group_by(year) %>% sample_frac(size = 0.15)

test_set <- test_set[order(test_set$pid),]


training_set <- stats_df[!stats_df$pid %in% test_set$pid,]


table(test_set$drafted)
table(training_set$drafted)
```
Now that we've split the data into test and training sets, we can remove the year column as it will not be used as a predictor in the model. Player id is also no longer necessary. Both of these columns will be removed from both the training set as well as the test set

```{r}
training_set <- training_set %>% ungroup()

test_set <- test_set %>% ungroup()

training_set <- dplyr::select(training_set, -c(pid,year))

test_set <- dplyr::select(test_set, -c(pid,year))

```

The two classes we have (Drafted vs not drafted) are extremely imbalanced. Less than 3 percent of the players in out data get drafted. This is a problem because if left this way our models will incorrectly classify many of the drafted players as not drafted. It could even have a relatively low error rate by simply classifying every observation as not drafted. Something needs to be done about this.

In order to mitigate this problem we will employ sampling techniques using the ovun_sample() function. In this case we will both oversample and undersample. The minority class(drafted players) will be oversampled with replacement while the majority class (undrafted players) will be undersampled with replacement. We will keep the same number of datapoints as there were in the origional training set.

```{r}
set.seed(112)
balanced_train_set <- ovun.sample(drafted ~ ., data = training_set, method = "both", p=0.3,                             N=17658, seed = 1)$data

table(balanced_train_set$drafted)
```

\newpage

# Model Fitting

Our process for fitting the models will follow the following steps:

1. First we'll fit an initial models using parameters that seem intuitive.

2. Next, we'll use cross validation to tune those parameters to find the best fit

3. Then we will fit a model using the optimal parameters we found and analyze each of the models using some sort of visualization.


When using cross validation, we will do this by looping through a list of reasonable numbers for that parameter. We will fit a model on all of the training set with the exception of the fold being used as the validation set for that iteration. We'll record the validation error, or out-of-bag error in the case of the random forrest model, to estimate the test error. We will then use the value for the parameter that results in the lowest estimated test error when constructing our final, optimal model.

*Random Forest:*

Fit an initial random forrest model. Here I use mtry = 5 as it is close to the square root of the number of predictors we have. We'll use cross validation later to determine the optimal number for this.

```{r}
rf.bball = randomForest(drafted ~ conference + num_seasons + Min_per + usg + FTM + FT_per + twoPM + twoP_per + TPM + TP_per + adjoe + rimmade + rim_per + midmade + mid_per + dunksmade + dunk_per + stops + min_played + off_reb + def_reb + total_reb + ast + stl + blk + pts + height, data=balanced_train_set, mtry=5, ntree=300, importance=TRUE)


plot(rf.bball)

```

To cross validate we will loop through all the reasonable values for m. We fit a model with each of the m's and record the out-of-bag error rate. We also make note of the classification error for the true values because if we are trying to predict which players get drafted, it is important to correctly classify as many of the drafted players as possible. Since the proportion of players that get drafted is small, minimizing the overall error might not do the best job at correctly classifying as many of the drafted players as possible. 

```{r}
m = c()
tce = c()
fce = c()
oob = c()

ntree = 500
for (i in 2:15){
  rf = randomForest(drafted ~ conference + num_seasons + Min_per + usg + FTM + FT_per + twoPM + twoP_per + TPM + TP_per + adjoe + rimmade + rim_per + midmade + mid_per + dunksmade + dunk_per + stops + min_played + off_reb + def_reb + total_reb + ast + stl + blk + pts + height, data=balanced_train_set, mtry=i, ntree=ntree, importance=TRUE)
  
  m[i-1] <- i
  tce[i-1] <- rf$confusion[2,3]
  oob[i-1] <- rf$err.rate[ntree, 1]
  fce[i-1] <- rf$confusion[1,3]
}
err = data.frame(cbind(m=m, class_error_true = tce, class_error_false = fce, out_of_bag = oob))

small_err <- err %>% filter(out_of_bag==min(out_of_bag))

m_optimal = max(small_err$m)
```

It looks like 
We fit a model with this parameter and plot the results below.
```{r}
rf_opt = randomForest(drafted ~ conference + num_seasons + Min_per + usg + FTM + FT_per + twoPM + twoP_per + TPM + TP_per + adjoe + rimmade + rim_per + midmade + mid_per + dunksmade + dunk_per + stops + min_played + off_reb + def_reb + total_reb + ast + stl + blk + pts + height, data=balanced_train_set, mtry=m_optimal, ntree=ntree, importance=TRUE)

rf_opt

plot(rf_opt)
```

```{r, fig.height=12, fig.width=12}
varImpPlot(rf_opt)

```


*Logistic Regression:*

We'll Start by fitting a logistic regression model with all of the predictors.

```{r}
full_model <- glm(drafted ~ ., data = balanced_train_set, family = 'binomial')
summary(full_model)


```

We notice that there are definitely some predictors that are more useful than others. We will use backward stepwise selection to determine the best subset of predictors for our model.

```{r}
step_model <- full_model %>% stepAIC(trace = FALSE)
summary(step_model)

```

It looks like our code dropped a few of the predictors to make a better model. We notice that the AIC for the step_model is lower than it was in the full_model which is a good thing.

Next, we must determine which threshhold is best to use to make predictions. We'll loop through all of the reasonable threshholds and then record the true positive rate, false positive rate, and total error for each. We'll use these metrics to decide on which threshold is best.

```{r}
prob.training = predict(step_model, type="response")



t <- c()
tpr <- c()
fpr <- c()
error <- c()
  
for (i in 1:9){
  
  bal_trn_st_w_pred = balanced_train_set %>% mutate(pred_drafted=as.factor(ifelse(prob.training<=(i/10), "No", "Yes")))

  con_mat <- table(pred=bal_trn_st_w_pred$pred_drafted, true=balanced_train_set$drafted)
  
  t[i] <- i/10
  tpr[i] <- con_mat[2,2]/(con_mat[2,2] + con_mat[1,2])
  fpr[i] <- con_mat[2,1]/(con_mat[2,1] + con_mat[1,1])
  error[i] <- (con_mat[1, 2] + con_mat[2,1])/(con_mat[1, 2] + con_mat[2,1] + con_mat[2, 2] + con_mat[1,1])
}
err_logit = data.frame(cbind(t=t, true_pos_rate = tpr, false_pos_rate = fpr, total_error = error))

err_logit

small_err_logit <- err_logit %>% filter(total_error==min(total_error))

optimal_threshold = max(small_err_logit$t)
```

Constructing and ROC Curve

```{r}
pred = prediction(prob.training, balanced_train_set$drafted)

perf = performance(pred, measure="tpr", x.measure="fpr")

plot(perf, col=2, lwd=3, main="ROC curve")
abline(0,1)

```

*Boosted Trees*


```{r}
#Cross Validation
set.seed(123)

boosted_fit <- rpart(drafted ~., data = balanced_train_set, method = "class", xval = 10)

rpart.plot(boosted_fit, extra = 106, yesno = TRUE)

printcp(boosted_fit)
plotcp(boosted_fit)

```

```{r}
#Potential Pruning
new_prune_boosted <- prune(boosted_fit, 
                  cp = boosted_fit$cptable[which.min(boosted_fit$cptable[, "xerror"]), "CP"])
rm(boosted_fit)
rpart.plot(new_prune_boosted, extra = 106, yesno = TRUE)
```

```{r}
class.pred <- predict(new_prune_boosted, test_set, type = "class")
conf_matrix <- confusionMatrix(class.pred, test_set$drafted)

class.acc <- round(as.numeric(conf_matrix$overall[1]),4)
paste("Boosted Tree Accuracy:", class.acc,"%")
```






*K-Nearest-Neighbors*

```{r}
str(training_set)

table(training_set$num_seasons)

table(training_set$conference)
```
Before we begin, we have to do something about the categorical variables we are using as predictors. A k-nearest-neighbors model would not work with variables like conference and num_seasons as we currently have them. We will dummycode these variables in both the training set as well as the test set.

```{r}
conference <- as.data.frame(dummy.code(balanced_train_set$conference))
num_seasons <- as.data.frame(dummy.code(balanced_train_set$num_seasons))

conference_t <- as.data.frame(dummy.code(test_set$conference))
num_seasons_t <- as.data.frame(dummy.code(test_set$num_seasons))


trn_st <- cbind(balanced_train_set, conference, num_seasons)

trn_st <- trn_st %>% dplyr::select(-one_of(c("conference", "num_seasons","6")))



tst_st <- cbind(test_set, conference_t, num_seasons_t)

tst_st <- tst_st %>% dplyr::select(-one_of(c("conference", "num_seasons", "6")))
```


```{r}

# YTrain is the true labels for drafted on the training set, XTrain is the design matrix
YTrain = trn_st$drafted
XTrain = trn_st %>% dplyr::select(-drafted) %>% scale(center = TRUE, scale = TRUE)
# YTest is the true labels for drafted on the test set, Xtest is the design matrix
YTest = tst_st$drafted
XTest = tst_st %>% dplyr::select(-drafted) %>% scale(center = TRUE, scale = TRUE)


```




```{r}
do.chunk <- function(chunkid, folddef, Xdat, Ydat, ...){
  # Get training index
  train = (folddef!=chunkid)
  # Get training set by the above index
  Xtr = Xdat[train,]
  # Get responses in training set
  Ytr = Ydat[train]
  # Get validation set
  Xvl = Xdat[!train,]
  # Get responses in validation set
  Yvl = Ydat[!train]
  # Predict training labels
  predYtr = knn(train=Xtr, test=Xtr, cl=Ytr, ...)
  # Predict validation labels
  predYvl = knn(train=Xtr, test=Xvl, cl=Ytr, ...)
  data.frame(fold = chunkid,
  train.error = mean(predYtr != Ytr), # Training error for each fold
  val.error = mean(predYvl != Yvl)) # Validation error for each fold
}

```



Here we execute the cross validation using the do.chunk function we created above. We will take the averages of the validation error for each value for k and store the lowest on as a variable called k_optimal
```{r}
nfold = 5
folds = cut(1:nrow(trn_st), breaks=nfold, labels=FALSE) %>% sample()



error.folds = NULL
# Give possible number of nearest neighbours to be considered
allK = 2:8
# Loop through different number of neighbors
for (k in allK){
# Loop through different chunk id
  for (j in seq(nfold)){
    tmp = do.chunk(chunkid=j, folddef=folds, Xdat=XTrain, Ydat=YTrain, k=k)
    tmp$neighbors = k # Record the last number of neighbor
    error.folds = rbind(error.folds, tmp) # combine results
  }
}
head(error.folds, 10)


smallest_err <- error.folds %>% group_by(neighbors) %>% summarise(neighbors = neighbors, avg_val.err = mean(val.error)) %>% ungroup() %>% filter(avg_val.err==min(avg_val.err))

k_optimal = max(smallest_err$neighbors)

```

This values for k_optimal is what we will use to evaluate the performance of this model and compare this k-nearest-neighbor model to the other models that we fit.


\newpage

# Model perforemance and Selection

Make predictions of the draft status of players in the test set using the Random Forrest model. Print a confusion matrix and record the test error

```{r}
yhat.rf = predict (rf_opt, newdata = test_set)
rf.err = table(pred = yhat.rf, truth = test_set$drafted)
test.rf.err = 1 - round(sum(diag(rf.err))/sum(rf.err), 6)
rf.err
print(paste('Random Forrest Error: ', 100*test.rf.err, '%'))
```

Now we will make predictions using the logistic regression model

```{r}
prob.test = predict(step_model, newdata = test_set, type="response")
test_set_w_pred = test_set %>% mutate(pred_drafted=as.factor(ifelse(prob.test<=(optimal_threshold), 0, 1)))
log.err <- table(pred = test_set_w_pred$pred_drafted, true = test_set$drafted)
test.log.err = round(1 - sum(diag(log.err))/sum(log.err), 6)
log.err
print(paste('Logistic Regression Error: ', 100*test.log.err, '%'))
```


```{r}
class.pred <- predict(new_prune_boosted, test_set, type = "class")
boost.err <- table(pred = class.pred, true = test_set$drafted)
test.boost.err = 1 - round(sum(diag(boost.err))/sum(boost.err), 6)
boost.err
paste("Boosted Tree error: ", 100*test.boost.err,"%")
```




```{r}
predYtst = knn(train=XTrain, test=XTest, cl=YTrain, k = k_optimal)
test.error = round(mean(predYtst != YTest), 6)
knn.err <- table(pred=predYtst, true=test_set$drafted)
knn.err
print(paste('K-Nearest-Neighbor Error: ', 100 * test.error, '%'))
```

The model with the lowest error when making predictions for the test set is the random Forrest model. HEre is a plot that shows the importance of each predictor in the model.

```{r, fig.width=12}
varImpPlot(rf_opt)

```

\newpage

# Conclusion

The model with the lowest error when making predictions for the test set is the random Forrest model. That model correctly classified 56 of the 92 players in the test set who would be drafted. It also correctly classified 3007 of the 3025 players who were not drafted. of the 74 players from the test set that our model predicted would be drafted, 56 of them actually would be drafted.

The logistic regression model had the second lowest error. This model actually had correctly identified more of the players that went on to be drafted. The reason that the test error is higher is that it classified over 100 players as drafted who would ultimately not get drafted. This likely has to do with our threshold. Had the threshold been set higher we would have seen more players incorrectly classified as undrafted and less players incorrectly classified as drafted. This could be tweaked to reflect what we find most important. If our goal is to correctly classify as many drafted players, we could slightly lower the threshold. If we instead were more focussed on maximizing the likelihood that a player that we classify as drafted actually gets drafted we could slightly raise the threshold. These tradeoffs can improve our model to better serve our goals for it.

The boosted decision tree model we fit had a test error rate of just over five percent. It suprised me that the boosted model did not perform better than the logistic regression model. In fact, it had more false positives and more false negatives. Overall, it isn't necessarily bad at predicting players' draft status, it's just that some of our other models were better. 

Lastly, the model with the highest error was the k-nearest-neighbors model with a test error of just over 10 percent. For this reason it is the least useful of our models. In addition, this model took the longest time and most computing power to run by far. In the future, I will most likely not use a k-nearest-neighbor model as a first choice when I encounter a classification problem. It was still good to see in this project how it performed compared to some of our other models.


