---
title: "Pstat 131 Final Project"
author: "Daniel Pry, Nick Van Daelen, Rick Zheng"
date: "2/27/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ISLR)
library(glmnet)
library(tree)
library(maptree)
library(randomForest)
library(gbm)
library(ROCR)
library(ggplot2)
```

### Pstat 131 Winter 2022 Final Project:

# Predicting if NCAA Basketball players will be drafted based on their stats.

# Introduction




# Loading Data

Loading Data Set:
source: https://www.kaggle.com/adityak2003/college-basketball-players-20092021
```{r}
college_stats <- read.csv('CollegeBasketballPlayers2009-2021.csv',
                          na.strings = c("", "N/A", "None", "-"))
```


# Data Cleaning

```{r}
stats <- college_stats %>% mutate(drafted = ifelse(is.na(pick), 0, 1))
```

```{r}
s <- stats %>% group_by(pid) %>% summarise(player_name = player_name, team = team, conf = conf, seasons = length(pid), Min_per  = Min_per, Ortg  = Ortg, usg  = usg, eFG  = eFG, TS_per  = TS_per, ORB_per  = ORB_per, DRB_per  = DRB_per, TO_per  = TO_per, AST_per  = AST_per, FTM  = FTM, FTA  = FTA, FT_per = FT_per, twoPM = twoPM, twoPA = twoPA, twoP_per = twoP_per, TPM = TPM, TPA = TPA, TP_per = TP_per, blk_per = blk_per, stl_per = stl_per, college_year = yr, ht = ht, adjoe  = adjoe, year = year, pid = pid, Rec.Rank = Rec.Rank, ast.tov = ast.tov, rimmade = rimmade, rim_att = rimmade.rimmiss, rim_per = rimmade..rimmade.rimmiss., midmade = midmade, mid_att = midmade.midmiss, mid_per = midmade..midmade.midmiss., dunksmade = dunksmade, dunks_att = dunksmiss.dunksmade, dunk_per = dunksmade..dunksmade.dunksmiss., drtg = drtg, adrtg = adrtg, stops = stops, bpm = bpm, min_played = mp, off_reb = oreb, def_reb = dreb, total_reb = treb, ast = ast, stl = stl, blk = blk, pts = pts, Position = X, drafted = drafted)

s_cleaned <- s %>% slice_max(year)
head(s_cleaned)
```

Cleaning abnormal values in data set
```{r}
s_cleaned[, 27][s_cleaned[, 27] == 0] <- NA
s_cleaned[, 27][s_cleaned[, 27] == "So"] <- NA
s_cleaned[, 27][s_cleaned[, 27] == "Jr"] <- NA
s_cleaned[, 26][s_cleaned[, 26] == 0] <- NA
```


```{r}
#changing ht
s_cleaned %>% select(ht)
begin_height <- data.frame(do.call("rbind", strsplit(as.character(s_cleaned$ht), "-", fixed = TRUE)))

```

```{r, fig.width=10}
begin_height["X1"][begin_height["X1"] == "Jun"] <- 72
begin_height["X1"][begin_height["X1"] == "Jul"] <- 84
begin_height["X1"][begin_height["X1"] == "May"] <- 60
begin_height["X2"][begin_height["X2"] == "Jun"] <- 72
begin_height["X2"][begin_height["X2"] == "Jul"] <- 84
begin_height["X2"][begin_height["X2"] == "May"] <- 60
begin_height$X1 <- as.numeric(as.character(begin_height$X1))
begin_height$X2 <- as.numeric(as.character(begin_height$X2))
begin_height$height <- begin_height$X1 + begin_height$X2

df_new <- cbind(s_cleaned, begin_height)

df_new %>% select(-c('X1', 'X2', 'ht'))
df_new$height <- df_new$height %>% as.numeric() %>% replace_na(76.27)
df_new %>% select(height)
s_cleaned <- df_new
```
Lastly, to finish cleaning our data we must get rid of players in the data who have not yet had the chance to either be drafted or to not be drafted. These players who are still playing in college will not help our model. A very good active college player would, as of now, have a zero in the "drafted" column. This could potentially weaken our model as they may go on to be drafted in the future. THerefore, we remove players whose "year" is 2021.

```{r}
s_cleaned <- subset(s_cleaned, year != 2021)


```

## Exploratory Data Analysis

```{r}
ggplot(s_cleaned, aes(x = pts)) + 
  geom_histogram()
```


Making box plots comparing the difference in the average value of certain predictors based on the player being drafted or not.
```{r}
# group by drafted or not
drafted_means <- s_cleaned %>% subset(drafted == 1) %>% sapply(mean, na.rm=TRUE)
drafted_means <- as.data.frame(drafted_means)

undrafted_means <- s_cleaned %>% subset(drafted == 0) %>% sapply(mean, na.rm=TRUE)
undrafted_means <- as.data.frame(undrafted_means)

# taking averages of drafted and undrafted players
drafted_vs_undrafted <- undrafted_means %>% mutate(drafted_means)

# transforming data frame for plotting
drafted_vs_undrafted$names <- rownames(drafted_vs_undrafted)
rownames(drafted_vs_undrafted) <- NULL
drafted_vs_undrafted <- gather(drafted_vs_undrafted, event, total, drafted_means:undrafted_means)

d_vs_u <- drafted_vs_undrafted %>% filter(names == "pts" | names == "dunksmade" |
                                            names == "min_played" | names == "ast" | 
                                            names == "TPM" | names == "midmade"| 
                                            names == "blk" | names == "total_reb" | 
                                            names == "twoPM" | names == "rimmade" |
                                            names == "stl" | names == "height")

# plotting of means
avg_bars <- d_vs_u %>% ggplot(aes(names, total, fill=event)) + 
  geom_bar(stat = "identity", position = 'dodge') + facet_wrap(~ names, scales = "free")
avg_bars
```


Making a barplot of the percentage of players from each conference that are drafted.

We notice that there are a handful of conferences that have a drastically higher percentage of players drafted. Additionally, there are a few conferences where no players have been drafted.
```{r, fig.width=10}
draft_by_conf <-  s_cleaned %>% group_by(conf) %>% summarise(prop_drafted = mean(drafted))

ggplot(draft_by_conf) + geom_bar(aes(x = conf, y = prop_drafted), stat = "identity") +
  theme(axis.text.x = element_text(angle = 90))
```

This plot exposes a few problems with the naming of the conferences in the Data. The independent conference is listed as 2 different conferences (ind and Ind) as a result of a discrepency in the capitalization and the PAC-12 was formerly known as the PAC-10 which is why there is a P10 as well as a P12. We fix these issues below and reprint the bar graph. 
```{r}
s_cleaned["conf"][s_cleaned["conf"] == "ind"] <- "Ind"
s_cleaned["conf"][s_cleaned["conf"] == "P10"] <- "P12"
```

```{r}
draft_by_conf <-  s_cleaned %>% group_by(conf) %>% summarise(prop_drafted = mean(drafted))

ggplot(draft_by_conf) + geom_bar(aes(x = conf, y = prop_drafted), stat = "identity") +
  theme(axis.text.x = element_text(angle = 90))
```

To better understand the relationship between our predictors, we make scatteplots with 2 predictors on the axis and color indicating wheter that player was drafted.

This first scatterplot has minutes played on the x axis and Points on the y axis. As expected these two variables are highly positively correlated with players who would go on to be drafted having higer values of both ending up in the top right of this plot.
```{r}
s_cleaned %>% ggplot(aes(x=min_played, y = pts, col = drafted)) + geom_point()

```

We made another scatterplot to examine the relationship between Dunks made and three pointers made. 

```{r}
s_cleaned %>% ggplot(aes(x=dunksmade, y = TPM, col = drafted)) + geom_point()


```

The metrics of assists (ast) and assist percentage are base off of similar stats and therefore we expect them to be highly correlated. We wonder if they should both be included in the model as predictors. We plot a scatterplot of the data on these two variables below.

```{r}

s_cleaned %>% ggplot(aes(x=AST_per, y = ast, col = drafted)) + geom_point()

```
From the scatterplot we can tell that the y axis (ast) is more informative to a players draft status than the x axis (ast_per) is. There are so many players that dont play all that often and could, as a result of sampling variation have a high assist percentage. None of these players wind up getting drafted. Therefore, we believe it is best to use assists as a predictor rather than assist percentage because the two metrics are highly correlated for players with a significant amount of data, but just looking at assist percentage could be misleading because of the players that aren't particularly good but still have a high assist percentage due to their small sample size.


```{r}
s_cleaned %>% ggplot(aes(x=rimmade, y = rim_per, col = drafted)) + geom_point()


```


# Test/training split

We start by simplifying our dataset to only include certain columns that will be used as predictors.

```{r}
sc <- s_cleaned
sc$conference <- as.factor(s_cleaned$conf)
sc$num_seasons <- as.factor(s_cleaned$seasons)
sc$drafted <- factor(s_cleaned$drafted, levels = c(0,1))


stats_df <- sc %>% select(c(pid, player_name, conference, num_seasons, year, Min_per, usg, FTM, FT_per, twoPM, twoP_per, TPM, TP_per, adjoe, rimmade, rim_per, midmade, mid_per, dunksmade, dunk_per, stops, min_played, off_reb, def_reb, total_reb, ast, stl, blk, pts, height, drafted))

```



Random forrests cannot contain kissing values in predictor columns, therefore the columns with many missing values (rimmade, midmade, dunksmade, rim_per, mid_per, and dunk_per) will have their missing values replaced with 0.

```{r}

sapply(stats_df, function(x) sum(is.na(x)))
```

```{r}

stats_df[is.na(stats_df)] <- 0

sapply(stats_df, function(x) sum(is.na(x)))

```


We will split the data into a test set and a training set. Since we have a reletively large number of observations, we should have a large enough test se if we used only 15 percent of the data for the test set, leaving many observations in the training set to fit our models.

We sample for our test set using a stratified random sample. This is a good idea in this case because ...

```{r}
set.seed(123)

test_set <- stats_df %>% group_by(year) %>% sample_frac(size = 0.15)

test_set <- test_set[order(test_set$pid),]


training_set <- stats_df[!stats_df$pid %in% test_set$pid,]


table(test_set$drafted)
table(training_set$drafted)
```


# Model Fitting

*Logistic Regression:*



*Random Forrest:*

Couldn't get it to work just yet. I think im close though


Fit an initial random forrest model. Here I use mtry = 5 as it is close to the square root of the number of predictors we have. We'll use cross validation later to determine the optimal number for this.

```{r}
rf.bball = randomForest(drafted ~ conference + num_seasons + Min_per + usg + FTM + FT_per + twoPM + twoP_per + TPM + TP_per + adjoe + rimmade + rim_per + midmade + mid_per + dunksmade + dunk_per + stops + min_played + off_reb + def_reb + total_reb + ast + stl + blk + pts + height, data=training_set, mtry=5, ntree=100, importance=TRUE)

rf.bball

plot(rf.bball)

predY = predict(rf.bball, newdata = training_set)

comb <- cbind.data.frame(pred = predY, actual = training_set$drafted)
comb$match <- comb$pred == comb$actual


sum(predY == 1 && training_set$drafted == 1)/sum(training_set$drafted == 1)
```

```{r}
do.chunk <- function(chunkid, folddef, dat, ...){
  # Get training index
  train = (folddef!=chunkid)
  # Get training set by the above index
  tr = dat[train,]

  # Get validation set
  vl = dat[!train,]

  
  rf = randomForest(drafted ~ conference + num_seasons + Min_per + usg + FTM + FT_per + twoPM + twoP_per + TPM + TP_per + adjoe + rimmade + rim_per + midmade + mid_per + dunksmade + dunk_per + stops + min_played + off_reb + def_reb + total_reb + ast + stl + blk + pts + height, data=dat, mtry=m, ntree=100, importance=TRUE)
  
  # Predict training labels
  predYtr = predict(rf, newdata = select(tr, -drafted))
  # Predict validation labels
  predYvl = predict(rf, newdata = select(vl, -drafted))
  
  data.frame(fold = chunkid, train.error = mean(predYtr != tr$drafted), train.tpr = sum(tr$drafted == 1 && predYtr == 1)/sum(tr$drafted == 1), val.error = mean(predYvl != vl$drafted)) # Validation error for each fold
}



nfold = 3

folds = cut(1:nrow(training_set), breaks=nfold, labels=FALSE) %>% sample()




error.folds = NULL
allM = 3:10
# Loop through different number for m
for (m in allM){
  # Loop through different chunk id
  for (j in seq(nfold)){
    tmp = do.chunk(chunkid=j, folddef=folds, dat = training_set, mtry=m)
    tmp$m = m # Record the last number of neighbor
    error.folds = rbind(error.folds, tmp) # combine results
  }
}
error.folds


```


*Bossted Trees:*



*K-Nearest-Neighbors*




# Model perforemance and Selection





# Conclusion
