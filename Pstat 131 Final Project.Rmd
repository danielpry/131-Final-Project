---
title: "Pstat 131 Final Project"
author: "Daniel Pry, Nick Van Daelen, Rick Zheng"
date: "2/27/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ISLR)
library(glmnet)
library(tree)
library(maptree)
library(randomForest)
library(gbm)
library(ROCR)
library(ggplot2)
library(ROSE)
```

### Pstat 131 Winter 2022 Final Project:

# Predicting if NCAA Basketball players will be drafted based on their stats.

# Introduction




# Loading Data

Loading Data Set:
source: https://www.kaggle.com/adityak2003/college-basketball-players-20092021
```{r}
college_stats <- read.csv('CollegeBasketballPlayers2009-2021.csv',
                          na.strings = c("", "N/A", "None", "-"))
```


# Data Cleaning

```{r}
stats <- college_stats %>% mutate(drafted = ifelse(is.na(pick), 0, 1))
```

```{r}
s <- stats %>% group_by(pid) %>% summarise(player_name = player_name, team = team, conf = conf, seasons = length(pid), Min_per  = Min_per, Ortg  = Ortg, usg  = usg, eFG  = eFG, TS_per  = TS_per, ORB_per  = ORB_per, DRB_per  = DRB_per, TO_per  = TO_per, AST_per  = AST_per, FTM  = FTM, FTA  = FTA, FT_per = FT_per, twoPM = twoPM, twoPA = twoPA, twoP_per = twoP_per, TPM = TPM, TPA = TPA, TP_per = TP_per, blk_per = blk_per, stl_per = stl_per, college_year = yr, ht = ht, adjoe  = adjoe, year = year, pid = pid, Rec.Rank = Rec.Rank, ast.tov = ast.tov, rimmade = rimmade, rim_att = rimmade.rimmiss, rim_per = rimmade..rimmade.rimmiss., midmade = midmade, mid_att = midmade.midmiss, mid_per = midmade..midmade.midmiss., dunksmade = dunksmade, dunks_att = dunksmiss.dunksmade, dunk_per = dunksmade..dunksmade.dunksmiss., drtg = drtg, adrtg = adrtg, stops = stops, bpm = bpm, min_played = mp, off_reb = oreb, def_reb = dreb, total_reb = treb, ast = ast, stl = stl, blk = blk, pts = pts, Position = X, drafted = drafted)

s_cleaned <- s %>% slice_max(year)
head(s_cleaned)
```

Cleaning abnormal values in data set
```{r}
s_cleaned[, 27][s_cleaned[, 27] == 0] <- NA
s_cleaned[, 27][s_cleaned[, 27] == "So"] <- NA
s_cleaned[, 27][s_cleaned[, 27] == "Jr"] <- NA
s_cleaned[, 26][s_cleaned[, 26] == 0] <- NA
```


```{r}
#changing ht
s_cleaned %>% select(ht)
begin_height <- data.frame(do.call("rbind", strsplit(as.character(s_cleaned$ht), "-", fixed = TRUE)))

```

```{r, fig.width=10}
begin_height["X1"][begin_height["X1"] == "Jun"] <- 72
begin_height["X1"][begin_height["X1"] == "Jul"] <- 84
begin_height["X1"][begin_height["X1"] == "May"] <- 60
begin_height["X2"][begin_height["X2"] == "Jun"] <- 72
begin_height["X2"][begin_height["X2"] == "Jul"] <- 84
begin_height["X2"][begin_height["X2"] == "May"] <- 60
begin_height$X1 <- as.numeric(as.character(begin_height$X1))
begin_height$X2 <- as.numeric(as.character(begin_height$X2))
begin_height$height <- begin_height$X1 + begin_height$X2

df_new <- cbind(s_cleaned, begin_height)

df_new %>% select(-c('X1', 'X2', 'ht'))
df_new$height <- df_new$height %>% as.numeric() %>% replace_na(76.27)
df_new %>% select(height)
s_cleaned <- df_new
```
Lastly, to finish cleaning our data we must get rid of players in the data who have not yet had the chance to either be drafted or to not be drafted. These players who are still playing in college will not help our model. A very good active college player would, as of now, have a zero in the "drafted" column. This could potentially weaken our model as they may go on to be drafted in the future. THerefore, we remove players whose "year" is 2021.

```{r}
s_cleaned <- subset(s_cleaned, year != 2021)


```

## Exploratory Data Analysis

```{r}
ggplot(s_cleaned, aes(x = pts)) + 
  geom_histogram()
```


Making box plots comparing the difference in the average value of certain predictors based on the player being drafted or not.
```{r}
# group by drafted or not
drafted_means <- s_cleaned %>% subset(drafted == 1) %>% sapply(mean, na.rm=TRUE)
drafted_means <- as.data.frame(drafted_means)

undrafted_means <- s_cleaned %>% subset(drafted == 0) %>% sapply(mean, na.rm=TRUE)
undrafted_means <- as.data.frame(undrafted_means)

# taking averages of drafted and undrafted players
drafted_vs_undrafted <- undrafted_means %>% mutate(drafted_means)

# transforming data frame for plotting
drafted_vs_undrafted$names <- rownames(drafted_vs_undrafted)
rownames(drafted_vs_undrafted) <- NULL
drafted_vs_undrafted <- gather(drafted_vs_undrafted, event, total, drafted_means:undrafted_means)

d_vs_u <- drafted_vs_undrafted %>% filter(names == "pts" | names == "dunksmade" |
                                            names == "min_played" | names == "ast" | 
                                            names == "TPM" | names == "midmade"| 
                                            names == "blk" | names == "total_reb" | 
                                            names == "twoPM" | names == "rimmade" |
                                            names == "stl" | names == "height")

# plotting of means
avg_bars <- d_vs_u %>% ggplot(aes(names, total, fill=event)) + 
  geom_bar(stat = "identity", position = 'dodge') + facet_wrap(~ names, scales = "free")
avg_bars
```


Making a barplot of the percentage of players from each conference that are drafted.

We notice that there are a handful of conferences that have a drastically higher percentage of players drafted. Additionally, there are a few conferences where no players have been drafted.
```{r, fig.width=10}
draft_by_conf <-  s_cleaned %>% group_by(conf) %>% summarise(prop_drafted = mean(drafted))

ggplot(draft_by_conf) + geom_bar(aes(x = conf, y = prop_drafted), stat = "identity") +
  theme(axis.text.x = element_text(angle = 90))
```

This plot exposes a few problems with the naming of the conferences in the Data. The independent conference is listed as 2 different conferences (ind and Ind) as a result of a discrepency in the capitalization and the PAC-12 was formerly known as the PAC-10 which is why there is a P10 as well as a P12. We fix these issues below and reprint the bar graph. 
```{r}
s_cleaned["conf"][s_cleaned["conf"] == "ind"] <- "Ind"
s_cleaned["conf"][s_cleaned["conf"] == "P10"] <- "P12"
```

```{r}
draft_by_conf <-  s_cleaned %>% group_by(conf) %>% summarise(prop_drafted = mean(drafted))

ggplot(draft_by_conf) + geom_bar(aes(x = conf, y = prop_drafted), stat = "identity") +
  theme(axis.text.x = element_text(angle = 90))
```

To better understand the relationship between our predictors, we make scatteplots with 2 predictors on the axis and color indicating wheter that player was drafted.

This first scatterplot has minutes played on the x axis and Points on the y axis. As expected these two variables are highly positively correlated with players who would go on to be drafted having higer values of both ending up in the top right of this plot.
```{r}
s_cleaned %>% ggplot(aes(x=min_played, y = pts, col = drafted)) + geom_point()

```

We made another scatterplot to examine the relationship between Dunks made and three pointers made. 

```{r}
s_cleaned %>% ggplot(aes(x=dunksmade, y = TPM, col = drafted)) + geom_point()


```

The metrics of assists (ast) and assist percentage are base off of similar stats and therefore we expect them to be highly correlated. We wonder if they should both be included in the model as predictors. We plot a scatterplot of the data on these two variables below.

```{r}

s_cleaned %>% ggplot(aes(x=AST_per, y = ast, col = drafted)) + geom_point()

```
From the scatterplot we can tell that the y axis (ast) is more informative to a players draft status than the x axis (ast_per) is. There are so many players that dont play all that often and could, as a result of sampling variation have a high assist percentage. None of these players wind up getting drafted. Therefore, we believe it is best to use assists as a predictor rather than assist percentage because the two metrics are highly correlated for players with a significant amount of data, but just looking at assist percentage could be misleading because of the players that aren't particularly good but still have a high assist percentage due to their small sample size.


```{r}
s_cleaned %>% ggplot(aes(x=rimmade, y = rim_per, col = drafted)) + geom_point()


```


# Test/training split

In order to simplify our code when we fit our models later, we start by simplifying our dataset to only include certain columns that will be used as predictors. Additionaly, categorical predictors such as conference, seasons, and drafted must be converted to factors.

```{r}
sc <- s_cleaned
sc$conference <- as.factor(s_cleaned$conf)
sc$num_seasons <- as.factor(s_cleaned$seasons)
sc$drafted <- factor(s_cleaned$drafted, levels = c(0,1))


stats_df <- sc %>% ungroup() %>% select(c(pid, year, conference, num_seasons, Min_per, usg, FTM, FT_per, twoPM, twoP_per, TPM, TP_per, adjoe, rimmade, rim_per, midmade, mid_per, dunksmade, dunk_per, stops, min_played, off_reb, def_reb, total_reb, ast, stl, blk, pts, height, drafted))

```



Random forrests cannot contain kissing values in predictor columns, therefore the columns with many missing values (rimmade, midmade, dunksmade, rim_per, mid_per, and dunk_per) will have their missing values replaced with 0.

```{r}

sapply(stats_df, function(x) sum(is.na(x)))
```

```{r}

stats_df[is.na(stats_df)] <- 0

sapply(stats_df, function(x) sum(is.na(x)))

```


We will split the data into a test set and a training set. Since we have a reletively large number of observations, we should have a large enough test se if we used only 15 percent of the data for the test set, leaving many observations in the training set to fit our models.

We sample for our test set using a stratified random sample. This is a good idea in this case because ...

```{r}
set.seed(123)

test_set <- stats_df %>% group_by(year) %>% sample_frac(size = 0.15)

test_set <- test_set[order(test_set$pid),]


training_set <- stats_df[!stats_df$pid %in% test_set$pid,]


table(test_set$drafted)
table(training_set$drafted)
```
Now that we've split the data into test and training sets, we can remove the year column as it will not be used as a predictor in the model. Player id is also no longer necessary. Both of these columns will be removed from both the training set as well as the test set

```{r}
training_set <- training_set %>% ungroup()

test_set <- test_set %>% ungroup()

training_set <- select(training_set, -c(pid,year))

test_set <- select(test_set, -c(pid,year))

```

The two classes we have (Drafted vs not drafted) are extremely imbalanced. Less than 3 percent of the players in out data get drafted. This is a problem because if left this way our models will incorrectly classify many of the drafted players as not drafted. It could even have a relatively low error rate by simply classifying every observation as not drafted. Something needs to be done about this.

In order to mitigate this problem we will employ sampling techniques using the ovun_sample() function. In this case we will both oversample and undersample. THe minority class(drafted players) will be oversampled with replacement while the majority class (undrafted players) will be undersampled with replacement. We will keep the same number of datapoints as there were in the origional training set.

```{r}
balanced_train_set <- ovun.sample(drafted ~ ., data = training_set, method = "both", p=0.2,                             N=17658, seed = 1)$data

table(balanced_train_set$drafted)
```

# Model Fitting


*Random Forrest:*


Fit an initial random forrest model. Here I use mtry = 5 as it is close to the square root of the number of predictors we have. We'll use cross validation later to determine the optimal number for this.

```{r}
rf.bball = randomForest(drafted ~ conference + num_seasons + Min_per + usg + FTM + FT_per + twoPM + twoP_per + TPM + TP_per + adjoe + rimmade + rim_per + midmade + mid_per + dunksmade + dunk_per + stops + min_played + off_reb + def_reb + total_reb + ast + stl + blk + pts + height, data=balanced_train_set, mtry=5, ntree=100, importance=TRUE)

class_err_true = rf.bball$confusion[2,3]
class_err_false = rf.bball$confusion[1,3]
oob.error = rf.bball$err.rate[100,1]

plot(rf.bball)


```

To cross validate we will loop through all the reasonable values for m. We fit a model with each of the m's and record the out-of-bag error rate. We also make note of the classification error for the true values because if we are trying to predict which players get drafted, it is important to correctly classify as many of the drafted players as possible. Since the proportion of players that get drafted is small, minimizing the overall error might not do the bast job at correctly classifying as many of the drafted players as possible. 

```{r}
m = c()
tce = c()
fce = c()
oob = c()

ntree = 300
for (i in 2:12){
  rf = randomForest(drafted ~ conference + num_seasons + Min_per + usg + FTM + FT_per + twoPM + twoP_per + TPM + TP_per + adjoe + rimmade + rim_per + midmade + mid_per + dunksmade + dunk_per + stops + min_played + off_reb + def_reb + total_reb + ast + stl + blk + pts + height, data=balanced_train_set, mtry=i, ntree=ntree, importance=TRUE)
  
  m[i-1] <- i
  tce[i-1] <- rf$confusion[2,3]
  oob[i-1] <- rf$err.rate[ntree, 1]
  fce[i-1] <- rf$confusion[1,3]
}
err = cbind(m=m, class_error_true = tce, class_error_false = fce, out_of_bag = oob)
```

It looks like 
We fit a model with this parameter and plot the results below.
```{r}
rf_opt = randomForest(drafted ~ conference + num_seasons + Min_per + usg + FTM + FT_per + twoPM + twoP_per + TPM + TP_per + adjoe + rimmade + rim_per + midmade + mid_per + dunksmade + dunk_per + stops + min_played + off_reb + def_reb + total_reb + ast + stl + blk + pts + height, data=training_set, mtry=8, ntree=300, importance=TRUE)

rf_opt

plot(rf_opt)

```


*Logistic Regression:*





*Bossted Trees:*





*K-Nearest-Neighbors*




# Model perforemance and Selection





# Conclusion
